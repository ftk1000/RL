# RL

* [Kaggle ref to Sutton's and Barton's book in PDF , HTML , and Py code](https://www.kaggle.com/c/google-football/discussion/187658)<br>
* [Py replication for Sutton & Barto's book Reinforcement Learning: An Introduction (2nd Edition) on GitHub](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)<br>
* [reinforcement-q-learning-scratch-python-openai-gym/](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)<br>
* [https://gym.openai.com/docs/](https://gym.openai.com/docs/)<br>


## [Steve Brunton: RL: ML and Control Theory](https://www.youtube.com/watch?v=0MNVhXEX9to)
     - Labels (reward per action) are extremely rare: semi-supervised learning: reward = time-delayed label.
     - Complex optimization problem: requires lots of data (trial and error)
     - \pi(s,a) = P(a|s)
     - good to know VALUE of policy \pi in each state
     - GOAL: optimize \pi(s,a) to maximize FUTURE REWARD
     - Assume ENVIRONMENT = Markov Decision Process
     - Q-Learning combines Value Function and Policy Learning
     - Credit Assignment Process (Minsky ~1960)
       -- Dense vs Sparse Reward
       -- Sample Efficiency
       -- Reward shaping (approximate intermidiate rewards)
       
      


## TD
* [TD Intro: In this article, we will be talking about TD(λ), which is a generic reinforcement learning method that unifies both Monte Carlo simulation and 1-step TD method.](https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60)<br>
[]()<br>
[]()<br>

## Andre Violante
* [simple-reinforcement-learning-q-learning](https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56)<br>
* [simple-reinforcement-learning-temporal-difference-learning](https://towardsdatascience.com/simple-reinforcement-learning-temporal-difference-learning-53d1b3263d79)<br>
* [distributed-deep-learning-pipelines-with-pyspark-and-keras](https://towardsdatascience.com/distributed-deep-learning-pipelines-with-pyspark-and-keras-a3a1c22b9239)<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>

# from ods@Slack
* [2020: Matthew Mahowald:  Playing TicTacToe with a DQN, redux](https://mahowald.github.io/pytorch-dqn/)<br>
* [github.com/yanji84/tic-tac-toe-rl](https://github.com/yanji84/tic-tac-toe-rl)<br>
* [github.com/abstractpaper/tictactoe-pytorch](https://github.com/abstractpaper/tictactoe-pytorch)<br>
* [2017: Arthur Juliani: Введение в обучение с подкреплением: от многорукого бандита до полноценного RL агента](https://habr.com/en/company/newprolab/blog/343834/)<br>
[]()<br>
[]()<br>
[]()<br>



























    echo "# RL" >> README.md
    git init
    git add README.md
    git commit -m "first commit"
    git remote add origin https://github.com/ftk1000/RL.git
    git push -u origin master
    
    
